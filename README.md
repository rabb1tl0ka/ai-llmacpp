# ğŸ§  ai-llmacpp

> A lightweight launcher for running `llama.cpp` models locally with enhanced usability and flexible shell scripts 
> for macOS, Linux, and Windows.

![Shell](https://img.shields.io/badge/shell-Bash-blue)
![Platform](https://img.shields.io/badge/platform-macOS%20|%20Linux%20|%20Windows-green)
![License](https://img.shields.io/badge/license-MIT-lightgrey)

---

## ğŸš€ Features

- âœ… Quick CLI launcher for `llama.cpp` models
- ğŸ” Model prefix matching + interactive selection
- ğŸ–¥ï¸  macOS/Linux support with Metal or multi-core CPU
- ğŸªŸ Windows support with optional DirectML acceleration
- ğŸ“ Ignores large model files while keeping folder structure
- ğŸ§¼ Clean and portable â€” no Python or extra deps required

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ kingabzpro/ (example)
â”‚   â”œâ”€â”€ mistral/    (example)
â”‚   â”œâ”€â”€ phi3/       (example)
â”œâ”€â”€ run-llama-universal.sh     # macOS + Linux runner
â”œâ”€â”€ run-llama-macos.sh         # macOS specific runner
â”œâ”€â”€ run-llama-linux.sh         # Linux specific runner
â”œâ”€â”€ run-llama-windows.bat      # Windows runner (PowerShell inside)
â”œâ”€â”€ setup_llama_mac.sh         # macOS setup script (Metal support)
â”œâ”€â”€ setup_llama_windows_advanced.bat  # Windows setup (DirectML or CPU)
```

---

## ğŸ› ï¸ Setup

### macOS

```bash
bash setup_llama_mac.sh
```

### Linux

```bash
# Manually build llama.cpp or use the universal script
make -j LLAMA_OPENBLAS=1
```

### Windows

Run from Git Bash or CMD:

```bat
setup_llama_windows_advanced.bat
```

---

## ğŸ§  Run it

Run from your terminal:

```bash
./run-llama-linux-mac.sh mistral "Explain RAG in simple terms"
```

If multiple models match, you'll be prompted to pick one.

---

## ğŸ“¦ Dependencies

- `llama.cpp` compiled locally
- macOS: Metal support enabled (`LLAMA_METAL=1`)
- Linux: OpenBLAS (`LLAMA_OPENBLAS=1`)
- Windows: Visual Studio + optional DirectML

---

## ğŸ“„ License

MIT â€” free to use, modify, and share.
